{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "import mlflow\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"vehicles_train\", {}, \"/home/forssh/workspace/Vehicle detection.v15i.coco/train/_annotations.coco.json\", \"/home/forssh/workspace/Vehicle detection.v15i.coco/train\")\n",
    "register_coco_instances(\"v\", {}, \"/home/forssh/workspace/Vehicle detection.v15i.coco/test/_annotations.coco.json\", \"/home/forssh/workspace/Vehicle detection.v15i.coco/test\")\n",
    "register_coco_instances(\"vehicles_val\", {}, \"/home/forssh/workspace/Vehicle detection.v15i.coco/valid/_annotations.coco.json\", \"/home/forssh/workspace/Vehicle detection.v15i.coco/valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg, CfgNode\n",
    "import mlflow\n",
    "cfg = get_cfg()\n",
    "\n",
    "cfg.MLFLOW = CfgNode()\n",
    "cfg.MLFLOW.EXPERIMENT_NAME = \"Vehicle Object Detection\"\n",
    "cfg.MLFLOW.RUN_DESCRIPTION = \"training with 10000 iterations, 3k images\"\n",
    "cfg.MLFLOW.RUN_NAME = \"#14 training\" # TODO: Исправить на автосмену\n",
    "cfg.MLFLOW.TRACKING_URI = \"sqlite:///mlruns.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n"
     ]
    }
   ],
   "source": [
    "%env MLFLOW_TRACKING_URI=sqlite:///mlruns.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import HookBase\n",
    "\n",
    "\n",
    "class MLflowHook(HookBase):\n",
    "    \"\"\"\n",
    "    A custom hook class that logs artifacts, metrics, and parameters to MLflow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg.clone()\n",
    "\n",
    "    def before_train(self):\n",
    "        with torch.no_grad():\n",
    "            mlflow.set_tracking_uri(self.cfg.MLFLOW.TRACKING_URI)\n",
    "            mlflow.set_experiment(self.cfg.MLFLOW.EXPERIMENT_NAME)\n",
    "            mlflow.start_run(run_name=self.cfg.MLFLOW.RUN_NAME)\n",
    "            mlflow.set_tag(\"mlflow.note.content\",\n",
    "                           self.cfg.MLFLOW.RUN_DESCRIPTION)\n",
    "            for k, v in self.cfg.items():\n",
    "                mlflow.log_param(k, v)\n",
    "\n",
    "    def after_step(self):\n",
    "        with torch.no_grad():\n",
    "            latest_metrics = self.trainer.storage.latest()\n",
    "            for k, v in latest_metrics.items():\n",
    "                mlflow.log_metric(key=k, value=v[0], step=v[1])\n",
    "\n",
    "    def after_train(self):\n",
    "        with torch.no_grad():\n",
    "            with open(os.path.join(self.cfg.OUTPUT_DIR, \"model-config.yaml\"), \"w\") as f:\n",
    "                f.write(self.cfg.dump())\n",
    "            mlflow.log_artifacts(self.cfg.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "class CocoTrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    A custom trainer class that evaluates the model on the validation set every `_C.TEST.EVAL_PERIOD` iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(cfg.OUTPUT_DIR_VALIDATION_SET_EVALUATION,\n",
    "                        exist_ok=True)\n",
    "        \n",
    "        return COCOEvaluator(dataset_name, distributed=False, output_dir=cfg.OUTPUT_DIR_VALIDATION_SET_EVALUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"vehicles_train\",)\n",
    "cfg.DATASETS.TEST= (\"vehicles_val\",)\n",
    "cfg.OUTPUT_DIR = (\"14_output\") # TODO: Исправить на автосмену\n",
    "cfg.OUTPUT_DIR_VALIDATION_SET_EVALUATION = os.path.join(\n",
    "        cfg.OUTPUT_DIR, \"validation-set-evaluation\")\n",
    "cfg.OUTPUT_DIR_TEST_SET_EVALUATION = os.path.join(\n",
    "        cfg.OUTPUT_DIR, \"test-set-evaluation\")\n",
    "cfg.TEST.EVAL_PERIOD = 100\n",
    "cfg.DATALOADER.NUM_WORKERS = 20\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 20  # This is the real \"batch size\" commonly known to deep learning people\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 20000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 9  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "cfg.SEED = 42\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(cfg.OUTPUT_DIR_VALIDATION_SET_EVALUATION, exist_ok=True)\n",
    "os.makedirs(cfg.OUTPUT_DIR_TEST_SET_EVALUATION, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/10 12:21:57 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/10 12:21:58 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[03/10 12:21:58 d2.data.datasets.coco]: \u001b[0mLoaded 2294 images in COCO format from /home/forssh/workspace/Vehicle detection.v15i.coco/train/_annotations.coco.json\n",
      "\u001b[32m[03/10 12:21:58 d2.data.build]: \u001b[0mRemoved 22 images with no usable annotations. 2272 images left.\n",
      "\u001b[32m[03/10 12:21:58 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n",
      "\u001b[36m|  category  | #instances   |   category    | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:-------------:|:-------------|:----------:|:-------------|\n",
      "|    cars    | 0            |     bike      | 812          |    bus     | 838          |\n",
      "|    car     | 6060         | constructio.. | 394          | emergency  | 348          |\n",
      "| motorbike  | 1046         | personal mo.. | 574          | quad bike  | 356          |\n",
      "|   truck    | 564          |               |              |            |              |\n",
      "|   total    | 10992        |               |              |            |              |\u001b[0m\n",
      "\u001b[32m[03/10 12:21:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[03/10 12:21:58 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[03/10 12:21:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/10 12:21:58 d2.data.common]: \u001b[0mSerializing 2272 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/10 12:21:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.99 MiB\n",
      "\u001b[32m[03/10 12:21:58 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=20\n",
      "\u001b[32m[03/10 12:21:58 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/model_final_68b088.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (10, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (10,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (36, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (36,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/10 12:21:58 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/forssh/workspace/.venv/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/10 12:23:20 d2.utils.events]: \u001b[0m eta: 17:23:51  iter: 19  total_loss: 2.623  loss_cls: 2.179  loss_box_reg: 0.3328  loss_rpn_cls: 0.08042  loss_rpn_loc: 0.03065    time: 3.4127  last_time: 3.1315  data_time: 0.0862  last_data_time: 0.0341   lr: 4.9953e-06  max_mem: 28043M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 12:23:21.046777: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-10 12:23:21.087367: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-10 12:23:21.087394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-10 12:23:21.088773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-10 12:23:21.096429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/10 12:24:29 d2.utils.events]: \u001b[0m eta: 17:35:50  iter: 39  total_loss: 2.556  loss_cls: 2.071  loss_box_reg: 0.3543  loss_rpn_cls: 0.08578  loss_rpn_loc: 0.04039    time: 3.3313  last_time: 3.4514  data_time: 0.0381  last_data_time: 0.0418   lr: 9.9902e-06  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:25:39 d2.utils.events]: \u001b[0m eta: 17:34:47  iter: 59  total_loss: 2.279  loss_cls: 1.844  loss_box_reg: 0.3207  loss_rpn_cls: 0.08299  loss_rpn_loc: 0.03    time: 3.3582  last_time: 3.1429  data_time: 0.0356  last_data_time: 0.0362   lr: 1.4985e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:26:45 d2.utils.events]: \u001b[0m eta: 17:26:55  iter: 79  total_loss: 1.985  loss_cls: 1.507  loss_box_reg: 0.357  loss_rpn_cls: 0.08832  loss_rpn_loc: 0.04307    time: 3.3224  last_time: 3.1383  data_time: 0.0357  last_data_time: 0.0367   lr: 1.998e-05  max_mem: 28043M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/10 12:27:52 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[03/10 12:27:52 d2.data.datasets.coco]: \u001b[0mLoaded 300 images in COCO format from /home/forssh/workspace/Vehicle detection.v15i.coco/valid/_annotations.coco.json\n",
      "\u001b[32m[03/10 12:27:52 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n",
      "\u001b[36m|  category  | #instances   |   category    | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:-------------:|:-------------|:----------:|:-------------|\n",
      "|    cars    | 0            |     bike      | 109          |    bus     | 59           |\n",
      "|    car     | 1384         | constructio.. | 26           | emergency  | 56           |\n",
      "| motorbike  | 148          | personal mo.. | 86           | quad bike  | 40           |\n",
      "|   truck    | 70           |               |              |            |              |\n",
      "|   total    | 1978         |               |              |            |              |\u001b[0m\n",
      "\u001b[32m[03/10 12:27:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[03/10 12:27:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/10 12:27:52 d2.data.common]: \u001b[0mSerializing 300 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/10 12:27:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[03/10 12:27:52 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[03/10 12:27:52 d2.evaluation.evaluator]: \u001b[0mStart inference on 300 batches\n",
      "\u001b[32m[03/10 12:27:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/300. Dataloading: 0.0018 s/iter. Inference: 0.0739 s/iter. Eval: 0.0003 s/iter. Total: 0.0760 s/iter. ETA=0:00:21\n",
      "\u001b[32m[03/10 12:27:59 d2.evaluation.evaluator]: \u001b[0mInference done 76/300. Dataloading: 0.0011 s/iter. Inference: 0.0731 s/iter. Eval: 0.0052 s/iter. Total: 0.0796 s/iter. ETA=0:00:17\n",
      "\u001b[32m[03/10 12:28:04 d2.evaluation.evaluator]: \u001b[0mInference done 144/300. Dataloading: 0.0012 s/iter. Inference: 0.0731 s/iter. Eval: 0.0028 s/iter. Total: 0.0771 s/iter. ETA=0:00:12\n",
      "\u001b[32m[03/10 12:28:09 d2.evaluation.evaluator]: \u001b[0mInference done 212/300. Dataloading: 0.0012 s/iter. Inference: 0.0731 s/iter. Eval: 0.0020 s/iter. Total: 0.0763 s/iter. ETA=0:00:06\n",
      "\u001b[32m[03/10 12:28:14 d2.evaluation.evaluator]: \u001b[0mInference done 280/300. Dataloading: 0.0012 s/iter. Inference: 0.0730 s/iter. Eval: 0.0016 s/iter. Total: 0.0758 s/iter. ETA=0:00:01\n",
      "\u001b[32m[03/10 12:28:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:22.419282 (0.075998 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:28:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:21 (0.072981 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:28:16 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[03/10 12:28:16 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to 14_output/validation-set-evaluation/coco_instances_results.json\n",
      "\u001b[32m[03/10 12:28:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.11s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.41s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.011\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.061\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.087\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.092\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.112\n",
      "\u001b[32m[03/10 12:28:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.879 | 3.122  | 0.149  | 0.296 | 1.505 | 1.149 |\n",
      "\u001b[32m[03/10 12:28:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP    | category               | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------------------|:------|:-----------|:------|\n",
      "| cars       | nan   | bike                   | 0.006 | bus        | 0.096 |\n",
      "| car        | 7.601 | construction equipment | 0.085 | emergency  | 0.121 |\n",
      "| motorbike  | 0.001 | personal mobility      | 0.004 | quad bike  | 0.000 |\n",
      "| truck      | 0.000 |                        |       |            |       |\n",
      "\u001b[32m[03/10 12:28:17 d2.engine.defaults]: \u001b[0mEvaluation results for vehicles_val in csv format:\n",
      "\u001b[32m[03/10 12:28:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[03/10 12:28:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[03/10 12:28:17 d2.evaluation.testing]: \u001b[0mcopypaste: 0.8792,3.1217,0.1492,0.2956,1.5054,1.1487\n",
      "\u001b[32m[03/10 12:28:17 d2.utils.events]: \u001b[0m eta: 17:26:17  iter: 99  total_loss: 1.636  loss_cls: 1.119  loss_box_reg: 0.3351  loss_rpn_cls: 0.05765  loss_rpn_loc: 0.02233    time: 3.3018  last_time: 3.4106  data_time: 0.0356  last_data_time: 0.0350   lr: 2.4975e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:29:27 d2.utils.events]: \u001b[0m eta: 17:26:39  iter: 119  total_loss: 1.24  loss_cls: 0.7659  loss_box_reg: 0.3531  loss_rpn_cls: 0.08176  loss_rpn_loc: 0.03503    time: 3.3047  last_time: 3.1347  data_time: 0.0389  last_data_time: 0.0360   lr: 2.997e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:30:37 d2.utils.events]: \u001b[0m eta: 17:26:47  iter: 139  total_loss: 1.061  loss_cls: 0.5605  loss_box_reg: 0.3706  loss_rpn_cls: 0.06853  loss_rpn_loc: 0.03484    time: 3.3026  last_time: 3.1352  data_time: 0.0353  last_data_time: 0.0328   lr: 3.4965e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:31:46 d2.utils.events]: \u001b[0m eta: 17:25:44  iter: 159  total_loss: 0.9883  loss_cls: 0.4794  loss_box_reg: 0.3789  loss_rpn_cls: 0.05907  loss_rpn_loc: 0.03845    time: 3.3002  last_time: 3.1371  data_time: 0.0350  last_data_time: 0.0358   lr: 3.996e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:32:54 d2.utils.events]: \u001b[0m eta: 17:24:00  iter: 179  total_loss: 0.9029  loss_cls: 0.4409  loss_box_reg: 0.3433  loss_rpn_cls: 0.05821  loss_rpn_loc: 0.02902    time: 3.2885  last_time: 3.1381  data_time: 0.0369  last_data_time: 0.0402   lr: 4.4955e-05  max_mem: 28043M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/10 12:34:02 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[03/10 12:34:02 d2.data.datasets.coco]: \u001b[0mLoaded 300 images in COCO format from /home/forssh/workspace/Vehicle detection.v15i.coco/valid/_annotations.coco.json\n",
      "\u001b[32m[03/10 12:34:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[03/10 12:34:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/10 12:34:02 d2.data.common]: \u001b[0mSerializing 300 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/10 12:34:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[03/10 12:34:02 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[03/10 12:34:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 300 batches\n",
      "\u001b[32m[03/10 12:34:04 d2.evaluation.evaluator]: \u001b[0mInference done 11/300. Dataloading: 0.0006 s/iter. Inference: 0.0698 s/iter. Eval: 0.0005 s/iter. Total: 0.0709 s/iter. ETA=0:00:20\n",
      "\u001b[32m[03/10 12:34:09 d2.evaluation.evaluator]: \u001b[0mInference done 82/300. Dataloading: 0.0011 s/iter. Inference: 0.0697 s/iter. Eval: 0.0003 s/iter. Total: 0.0711 s/iter. ETA=0:00:15\n",
      "\u001b[32m[03/10 12:34:14 d2.evaluation.evaluator]: \u001b[0mInference done 153/300. Dataloading: 0.0011 s/iter. Inference: 0.0697 s/iter. Eval: 0.0003 s/iter. Total: 0.0711 s/iter. ETA=0:00:10\n",
      "\u001b[32m[03/10 12:34:19 d2.evaluation.evaluator]: \u001b[0mInference done 224/300. Dataloading: 0.0012 s/iter. Inference: 0.0696 s/iter. Eval: 0.0003 s/iter. Total: 0.0711 s/iter. ETA=0:00:05\n",
      "\u001b[32m[03/10 12:34:24 d2.evaluation.evaluator]: \u001b[0mInference done 295/300. Dataloading: 0.0011 s/iter. Inference: 0.0696 s/iter. Eval: 0.0003 s/iter. Total: 0.0711 s/iter. ETA=0:00:00\n",
      "\u001b[32m[03/10 12:34:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:21.064460 (0.071405 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:34:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:20 (0.069597 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:34:24 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[03/10 12:34:24 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to 14_output/validation-set-evaluation/coco_instances_results.json\n",
      "\u001b[32m[03/10 12:34:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.28s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.046\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.184\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.197\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.229\n",
      "\u001b[32m[03/10 12:34:26 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 3.332 | 6.154  | 3.031  | 1.956 | 4.749 | 4.568 |\n",
      "\u001b[32m[03/10 12:34:26 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category               | AP    | category   | AP    |\n",
      "|:-----------|:-------|:-----------------------|:------|:-----------|:------|\n",
      "| cars       | nan    | bike                   | 0.081 | bus        | 0.350 |\n",
      "| car        | 26.042 | construction equipment | 0.242 | emergency  | 3.105 |\n",
      "| motorbike  | 0.000  | personal mobility      | 0.000 | quad bike  | 0.169 |\n",
      "| truck      | 0.000  |                        |       |            |       |\n",
      "\u001b[32m[03/10 12:34:26 d2.engine.defaults]: \u001b[0mEvaluation results for vehicles_val in csv format:\n",
      "\u001b[32m[03/10 12:34:26 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[03/10 12:34:26 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[03/10 12:34:26 d2.evaluation.testing]: \u001b[0mcopypaste: 3.3321,6.1542,3.0309,1.9565,4.7489,4.5676\n",
      "\u001b[32m[03/10 12:34:26 d2.utils.events]: \u001b[0m eta: 17:22:57  iter: 199  total_loss: 0.8663  loss_cls: 0.4258  loss_box_reg: 0.3649  loss_rpn_cls: 0.04659  loss_rpn_loc: 0.03011    time: 3.2836  last_time: 3.1301  data_time: 0.0356  last_data_time: 0.0360   lr: 4.995e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:35:36 d2.utils.events]: \u001b[0m eta: 17:22:27  iter: 219  total_loss: 0.8435  loss_cls: 0.4024  loss_box_reg: 0.3656  loss_rpn_cls: 0.04601  loss_rpn_loc: 0.02754    time: 3.2875  last_time: 3.4685  data_time: 0.0385  last_data_time: 0.0393   lr: 5.4945e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:36:46 d2.utils.events]: \u001b[0m eta: 17:23:02  iter: 239  total_loss: 0.8026  loss_cls: 0.3732  loss_box_reg: 0.3381  loss_rpn_cls: 0.04752  loss_rpn_loc: 0.03025    time: 3.2908  last_time: 3.5844  data_time: 0.0391  last_data_time: 0.0395   lr: 5.994e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:37:56 d2.utils.events]: \u001b[0m eta: 17:24:13  iter: 259  total_loss: 0.7917  loss_cls: 0.3667  loss_box_reg: 0.3577  loss_rpn_cls: 0.03776  loss_rpn_loc: 0.02623    time: 3.2954  last_time: 2.9361  data_time: 0.0376  last_data_time: 0.0362   lr: 6.4935e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:39:12 d2.utils.events]: \u001b[0m eta: 17:27:39  iter: 279  total_loss: 0.7693  loss_cls: 0.3527  loss_box_reg: 0.3542  loss_rpn_cls: 0.03662  loss_rpn_loc: 0.02702    time: 3.3196  last_time: 3.4975  data_time: 0.0374  last_data_time: 0.0371   lr: 6.993e-05  max_mem: 28043M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/10 12:40:22 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[03/10 12:40:22 d2.data.datasets.coco]: \u001b[0mLoaded 300 images in COCO format from /home/forssh/workspace/Vehicle detection.v15i.coco/valid/_annotations.coco.json\n",
      "\u001b[32m[03/10 12:40:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[03/10 12:40:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/10 12:40:22 d2.data.common]: \u001b[0mSerializing 300 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/10 12:40:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[03/10 12:40:22 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[03/10 12:40:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 300 batches\n",
      "\u001b[32m[03/10 12:40:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/300. Dataloading: 0.0005 s/iter. Inference: 0.0697 s/iter. Eval: 0.0003 s/iter. Total: 0.0706 s/iter. ETA=0:00:20\n",
      "\u001b[32m[03/10 12:40:29 d2.evaluation.evaluator]: \u001b[0mInference done 82/300. Dataloading: 0.0011 s/iter. Inference: 0.0699 s/iter. Eval: 0.0003 s/iter. Total: 0.0713 s/iter. ETA=0:00:15\n",
      "\u001b[32m[03/10 12:40:34 d2.evaluation.evaluator]: \u001b[0mInference done 148/300. Dataloading: 0.0011 s/iter. Inference: 0.0723 s/iter. Eval: 0.0003 s/iter. Total: 0.0738 s/iter. ETA=0:00:11\n",
      "\u001b[32m[03/10 12:40:39 d2.evaluation.evaluator]: \u001b[0mInference done 219/300. Dataloading: 0.0012 s/iter. Inference: 0.0715 s/iter. Eval: 0.0003 s/iter. Total: 0.0730 s/iter. ETA=0:00:05\n",
      "\u001b[32m[03/10 12:40:44 d2.evaluation.evaluator]: \u001b[0mInference done 290/300. Dataloading: 0.0012 s/iter. Inference: 0.0710 s/iter. Eval: 0.0003 s/iter. Total: 0.0725 s/iter. ETA=0:00:00\n",
      "\u001b[32m[03/10 12:40:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:21.463064 (0.072756 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:40:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:20 (0.070994 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:40:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[03/10 12:40:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to 14_output/validation-set-evaluation/coco_instances_results.json\n",
      "\u001b[32m[03/10 12:40:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.75s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.29s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.077\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.037\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.075\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.102\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237\n",
      "\u001b[32m[03/10 12:40:48 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 3.271 | 7.654  | 1.649  | 3.183 | 3.728 | 4.090 |\n",
      "\u001b[32m[03/10 12:40:48 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category               | AP    | category   | AP    |\n",
      "|:-----------|:-------|:-----------------------|:------|:-----------|:------|\n",
      "| cars       | nan    | bike                   | 1.441 | bus        | 0.579 |\n",
      "| car        | 23.266 | construction equipment | 0.128 | emergency  | 3.703 |\n",
      "| motorbike  | 0.005  | personal mobility      | 0.000 | quad bike  | 0.313 |\n",
      "| truck      | 0.000  |                        |       |            |       |\n",
      "\u001b[32m[03/10 12:40:48 d2.engine.defaults]: \u001b[0mEvaluation results for vehicles_val in csv format:\n",
      "\u001b[32m[03/10 12:40:48 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[03/10 12:40:48 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[03/10 12:40:48 d2.evaluation.testing]: \u001b[0mcopypaste: 3.2707,7.6544,1.6489,3.1827,3.7282,4.0901\n",
      "\u001b[32m[03/10 12:40:48 d2.utils.events]: \u001b[0m eta: 17:28:05  iter: 299  total_loss: 0.8137  loss_cls: 0.358  loss_box_reg: 0.3685  loss_rpn_cls: 0.03643  loss_rpn_loc: 0.02879    time: 3.3202  last_time: 3.3019  data_time: 0.0368  last_data_time: 0.0367   lr: 7.4925e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:41:56 d2.utils.events]: \u001b[0m eta: 17:27:37  iter: 319  total_loss: 0.8143  loss_cls: 0.3517  loss_box_reg: 0.3853  loss_rpn_cls: 0.0281  loss_rpn_loc: 0.02658    time: 3.3170  last_time: 3.1462  data_time: 0.0381  last_data_time: 0.0365   lr: 7.992e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:43:05 d2.utils.events]: \u001b[0m eta: 17:26:15  iter: 339  total_loss: 0.7676  loss_cls: 0.3306  loss_box_reg: 0.3498  loss_rpn_cls: 0.03291  loss_rpn_loc: 0.03089    time: 3.3145  last_time: 3.1937  data_time: 0.0371  last_data_time: 0.0363   lr: 8.4915e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:44:16 d2.utils.events]: \u001b[0m eta: 17:26:37  iter: 359  total_loss: 0.7521  loss_cls: 0.3285  loss_box_reg: 0.366  loss_rpn_cls: 0.02898  loss_rpn_loc: 0.0275    time: 3.3160  last_time: 3.1127  data_time: 0.0371  last_data_time: 0.0353   lr: 8.991e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:45:25 d2.utils.events]: \u001b[0m eta: 17:25:33  iter: 379  total_loss: 0.7381  loss_cls: 0.3107  loss_box_reg: 0.3736  loss_rpn_cls: 0.02831  loss_rpn_loc: 0.02669    time: 3.3142  last_time: 3.1462  data_time: 0.0364  last_data_time: 0.0366   lr: 9.4905e-05  max_mem: 28043M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/10 12:46:35 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[03/10 12:46:35 d2.data.datasets.coco]: \u001b[0mLoaded 300 images in COCO format from /home/forssh/workspace/Vehicle detection.v15i.coco/valid/_annotations.coco.json\n",
      "\u001b[32m[03/10 12:46:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[03/10 12:46:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/10 12:46:35 d2.data.common]: \u001b[0mSerializing 300 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/10 12:46:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[03/10 12:46:35 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[03/10 12:46:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 300 batches\n",
      "\u001b[32m[03/10 12:46:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/300. Dataloading: 0.0004 s/iter. Inference: 0.0697 s/iter. Eval: 0.0003 s/iter. Total: 0.0704 s/iter. ETA=0:00:20\n",
      "\u001b[32m[03/10 12:46:42 d2.evaluation.evaluator]: \u001b[0mInference done 82/300. Dataloading: 0.0010 s/iter. Inference: 0.0697 s/iter. Eval: 0.0003 s/iter. Total: 0.0711 s/iter. ETA=0:00:15\n",
      "\u001b[32m[03/10 12:46:47 d2.evaluation.evaluator]: \u001b[0mInference done 153/300. Dataloading: 0.0011 s/iter. Inference: 0.0697 s/iter. Eval: 0.0003 s/iter. Total: 0.0712 s/iter. ETA=0:00:10\n",
      "\u001b[32m[03/10 12:46:52 d2.evaluation.evaluator]: \u001b[0mInference done 224/300. Dataloading: 0.0012 s/iter. Inference: 0.0697 s/iter. Eval: 0.0003 s/iter. Total: 0.0712 s/iter. ETA=0:00:05\n",
      "\u001b[32m[03/10 12:46:57 d2.evaluation.evaluator]: \u001b[0mInference done 290/300. Dataloading: 0.0012 s/iter. Inference: 0.0709 s/iter. Eval: 0.0003 s/iter. Total: 0.0724 s/iter. ETA=0:00:00\n",
      "\u001b[32m[03/10 12:46:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:21.433741 (0.072657 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:46:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:20 (0.070870 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:46:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[03/10 12:46:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to 14_output/validation-set-evaluation/coco_instances_results.json\n",
      "\u001b[32m[03/10 12:46:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.94s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.25s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.143\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.042\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.037\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.071\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.088\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.108\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.085\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.171\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.239\n",
      "\u001b[32m[03/10 12:47:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 6.434 | 14.282 | 4.189  | 3.671 | 7.090 | 8.832 |\n",
      "\u001b[32m[03/10 12:47:00 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category               | AP     | category   | AP    |\n",
      "|:-----------|:-------|:-----------------------|:-------|:-----------|:------|\n",
      "| cars       | nan    | bike                   | 22.498 | bus        | 0.729 |\n",
      "| car        | 27.019 | construction equipment | 0.151  | emergency  | 5.565 |\n",
      "| motorbike  | 0.558  | personal mobility      | 0.000  | quad bike  | 1.381 |\n",
      "| truck      | 0.000  |                        |        |            |       |\n",
      "\u001b[32m[03/10 12:47:00 d2.engine.defaults]: \u001b[0mEvaluation results for vehicles_val in csv format:\n",
      "\u001b[32m[03/10 12:47:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[03/10 12:47:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[03/10 12:47:00 d2.evaluation.testing]: \u001b[0mcopypaste: 6.4336,14.2822,4.1893,3.6707,7.0896,8.8325\n",
      "\u001b[32m[03/10 12:47:00 d2.utils.events]: \u001b[0m eta: 17:25:13  iter: 399  total_loss: 0.6947  loss_cls: 0.2934  loss_box_reg: 0.3445  loss_rpn_cls: 0.02539  loss_rpn_loc: 0.02401    time: 3.3145  last_time: 3.4945  data_time: 0.0367  last_data_time: 0.0365   lr: 9.99e-05  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:48:10 d2.utils.events]: \u001b[0m eta: 17:25:45  iter: 419  total_loss: 0.7625  loss_cls: 0.3045  loss_box_reg: 0.3777  loss_rpn_cls: 0.03462  loss_rpn_loc: 0.03852    time: 3.3138  last_time: 3.1552  data_time: 0.0389  last_data_time: 0.0402   lr: 0.0001049  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:49:20 d2.utils.events]: \u001b[0m eta: 17:25:26  iter: 439  total_loss: 0.73  loss_cls: 0.2883  loss_box_reg: 0.3722  loss_rpn_cls: 0.02913  loss_rpn_loc: 0.03043    time: 3.3143  last_time: 3.1560  data_time: 0.0376  last_data_time: 0.0392   lr: 0.00010989  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:50:30 d2.utils.events]: \u001b[0m eta: 17:25:52  iter: 459  total_loss: 0.7048  loss_cls: 0.2703  loss_box_reg: 0.3301  loss_rpn_cls: 0.03261  loss_rpn_loc: 0.03149    time: 3.3150  last_time: 3.4082  data_time: 0.0383  last_data_time: 0.0398   lr: 0.00011489  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:51:39 d2.utils.events]: \u001b[0m eta: 17:24:10  iter: 479  total_loss: 0.6837  loss_cls: 0.2686  loss_box_reg: 0.3376  loss_rpn_cls: 0.02964  loss_rpn_loc: 0.03053    time: 3.3135  last_time: 3.3024  data_time: 0.0385  last_data_time: 0.0370   lr: 0.00011988  max_mem: 28043M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/10 12:52:49 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[03/10 12:52:49 d2.data.datasets.coco]: \u001b[0mLoaded 300 images in COCO format from /home/forssh/workspace/Vehicle detection.v15i.coco/valid/_annotations.coco.json\n",
      "\u001b[32m[03/10 12:52:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[03/10 12:52:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/10 12:52:49 d2.data.common]: \u001b[0mSerializing 300 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/10 12:52:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[03/10 12:52:49 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[03/10 12:52:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 300 batches\n",
      "\u001b[32m[03/10 12:52:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/300. Dataloading: 0.0012 s/iter. Inference: 0.0723 s/iter. Eval: 0.0005 s/iter. Total: 0.0739 s/iter. ETA=0:00:21\n",
      "\u001b[32m[03/10 12:52:56 d2.evaluation.evaluator]: \u001b[0mInference done 79/300. Dataloading: 0.0019 s/iter. Inference: 0.0721 s/iter. Eval: 0.0003 s/iter. Total: 0.0745 s/iter. ETA=0:00:16\n",
      "\u001b[32m[03/10 12:53:01 d2.evaluation.evaluator]: \u001b[0mInference done 148/300. Dataloading: 0.0016 s/iter. Inference: 0.0718 s/iter. Eval: 0.0003 s/iter. Total: 0.0738 s/iter. ETA=0:00:11\n",
      "\u001b[32m[03/10 12:53:06 d2.evaluation.evaluator]: \u001b[0mInference done 217/300. Dataloading: 0.0015 s/iter. Inference: 0.0717 s/iter. Eval: 0.0003 s/iter. Total: 0.0736 s/iter. ETA=0:00:06\n",
      "\u001b[32m[03/10 12:53:11 d2.evaluation.evaluator]: \u001b[0mInference done 286/300. Dataloading: 0.0014 s/iter. Inference: 0.0716 s/iter. Eval: 0.0003 s/iter. Total: 0.0734 s/iter. ETA=0:00:01\n",
      "\u001b[32m[03/10 12:53:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:21.725037 (0.073644 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:53:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:21 (0.071616 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/10 12:53:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[03/10 12:53:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to 14_output/validation-set-evaluation/coco_instances_results.json\n",
      "\u001b[32m[03/10 12:53:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=2.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.23s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.091\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.187\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.083\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.097\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.131\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.231\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.247\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.109\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.283\n",
      "\u001b[32m[03/10 12:53:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 9.131 | 18.692 | 7.172  | 8.285 | 9.745 | 13.094 |\n",
      "\u001b[32m[03/10 12:53:15 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category               | AP     | category   | AP    |\n",
      "|:-----------|:-------|:-----------------------|:-------|:-----------|:------|\n",
      "| cars       | nan    | bike                   | 25.618 | bus        | 3.064 |\n",
      "| car        | 36.449 | construction equipment | 0.335  | emergency  | 7.414 |\n",
      "| motorbike  | 4.811  | personal mobility      | 0.000  | quad bike  | 4.490 |\n",
      "| truck      | 0.000  |                        |        |            |       |\n",
      "\u001b[32m[03/10 12:53:15 d2.engine.defaults]: \u001b[0mEvaluation results for vehicles_val in csv format:\n",
      "\u001b[32m[03/10 12:53:15 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[03/10 12:53:15 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[03/10 12:53:15 d2.evaluation.testing]: \u001b[0mcopypaste: 9.1311,18.6915,7.1724,8.2850,9.7447,13.0938\n",
      "\u001b[32m[03/10 12:53:15 d2.utils.events]: \u001b[0m eta: 17:24:01  iter: 499  total_loss: 0.6955  loss_cls: 0.2716  loss_box_reg: 0.3277  loss_rpn_cls: 0.02314  loss_rpn_loc: 0.03224    time: 3.3144  last_time: 3.1467  data_time: 0.0381  last_data_time: 0.0357   lr: 0.00012488  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:54:23 d2.utils.events]: \u001b[0m eta: 17:21:17  iter: 519  total_loss: 0.6357  loss_cls: 0.2625  loss_box_reg: 0.3206  loss_rpn_cls: 0.02312  loss_rpn_loc: 0.02521    time: 3.3117  last_time: 3.1416  data_time: 0.0390  last_data_time: 0.0381   lr: 0.00012987  max_mem: 28043M\n",
      "\u001b[32m[03/10 12:55:31 d2.utils.events]: \u001b[0m eta: 17:19:10  iter: 539  total_loss: 0.663  loss_cls: 0.2625  loss_box_reg: 0.3425  loss_rpn_cls: 0.0255  loss_rpn_loc: 0.02532    time: 3.3075  last_time: 2.9386  data_time: 0.0408  last_data_time: 0.0372   lr: 0.00013487  max_mem: 28043M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#setup_logger(output=os.path.join(cfg.OUTPUT_DIR, \"training-log.txt\"))\n",
    "\n",
    "mlflow_hook = MLflowHook(cfg)\n",
    "\n",
    "trainer = CocoTrainer(cfg)\n",
    "trainer.register_hooks(hooks=[mlflow_hook])\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/08 23:05:42 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from 13_output/model_final.pth ...\n"
     ]
    }
   ],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.65    # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/08 23:05:43 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from 13_output/model_final.pth ...\n",
      "\u001b[32m[03/08 23:05:43 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[03/08 23:05:43 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[03/08 23:05:43 d2.data.datasets.coco]: \u001b[0mLoaded 342 images in COCO format from /home/forssh/workspace/Vehicle detection.v15i.coco/test/_annotations.coco.json\n",
      "\u001b[32m[03/08 23:05:43 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n",
      "\u001b[36m|  category  | #instances   |   category    | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:-------------:|:-------------|:----------:|:-------------|\n",
      "|    cars    | 0            |     bike      | 161          |    bus     | 81           |\n",
      "|    car     | 1359         | constructio.. | 34           | emergency  | 53           |\n",
      "| motorbike  | 113          | personal mo.. | 84           | quad bike  | 75           |\n",
      "|   truck    | 73           |               |              |            |              |\n",
      "|   total    | 2033         |               |              |            |              |\u001b[0m\n",
      "\u001b[32m[03/08 23:05:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[03/08 23:05:43 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[03/08 23:05:43 d2.data.common]: \u001b[0mSerializing 342 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/08 23:05:43 d2.data.common]: \u001b[0mSerialized dataset takes 0.16 MiB\n",
      "\u001b[32m[03/08 23:05:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 342 batches\n",
      "\u001b[32m[03/08 23:05:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/342. Dataloading: 0.0007 s/iter. Inference: 0.0387 s/iter. Eval: 0.0003 s/iter. Total: 0.0397 s/iter. ETA=0:00:13\n",
      "\u001b[32m[03/08 23:05:50 d2.evaluation.evaluator]: \u001b[0mInference done 143/342. Dataloading: 0.0010 s/iter. Inference: 0.0368 s/iter. Eval: 0.0002 s/iter. Total: 0.0381 s/iter. ETA=0:00:07\n",
      "\u001b[32m[03/08 23:05:55 d2.evaluation.evaluator]: \u001b[0mInference done 275/342. Dataloading: 0.0011 s/iter. Inference: 0.0367 s/iter. Eval: 0.0002 s/iter. Total: 0.0380 s/iter. ETA=0:00:02\n",
      "\u001b[32m[03/08 23:05:58 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:12.892249 (0.038256 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/08 23:05:58 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.036652 s / iter per device, on 1 devices)\n",
      "\u001b[32m[03/08 23:05:58 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[03/08 23:05:58 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to 13_output/test-set-evaluation/coco_instances_results.json\n",
      "\u001b[32m[03/08 23:05:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.53s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.19s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.432\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.663\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.460\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.164\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.327\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.340\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.507\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.528\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.270\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590\n",
      "\u001b[32m[03/08 23:05:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 43.220 | 66.307 | 45.954 | 16.445 | 32.658 | 51.785 |\n",
      "\u001b[32m[03/08 23:05:59 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category               | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------------------|:-------|:-----------|:-------|\n",
      "| cars       | nan    | bike                   | 48.904 | bus        | 47.911 |\n",
      "| car        | 54.724 | construction equipment | 30.932 | emergency  | 79.053 |\n",
      "| motorbike  | 28.000 | personal mobility      | 55.439 | quad bike  | 44.015 |\n",
      "| truck      | 0.000  |                        |        |            |        |\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "\n",
    "#setup_logger(output=os.path.join(cfg.OUTPUT_DIR_TEST_SET_EVALUATION, \"evaluation-log.txt\"))\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "evaluator = COCOEvaluator(\"v\", output_dir=cfg.OUTPUT_DIR_TEST_SET_EVALUATION)\n",
    "test_set_loader = build_detection_test_loader(cfg, \"v\")\n",
    "\n",
    "evaluation_results = inference_on_dataset(predictor.model, test_set_loader, evaluator)\n",
    "#logging.info(\"Evaluation results on test set: %s\", evaluation_results)\n",
    "\n",
    "for k, v in evaluation_results[\"bbox\"].items():\n",
    "    mlflow.log_metric(f\"Test Set {k}\", v, step=0)\n",
    "\n",
    "mlflow.log_artifacts(cfg.OUTPUT_DIR_TEST_SET_EVALUATION, \"test-set-evaluation\")\n",
    "mlflow.log_text(str(evaluation_results), \"test-set-evaluation/coco-metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/09 03:38:29 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from 13_output/model_final.pth ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.65    # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "\n",
    "dataset_name = 'vehicles_test'\n",
    "dataset_metadata = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "\n",
    "input_images = [\n",
    "    \"Vehilce detection.v3i.coco/train/1ff7910c-16884491480894_jpeg.rf.70dbfcc5722372608719d20b68a71541.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/train/fe27f76c-electricScooter_114_jpg.rf.6dfaaaa049ee65ef79cb656a14d3a35f.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/train/f7ab2b5f-52_jpg.rf.93d3972b5242280d38f273f74a001319.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/train/d951a446-i_jpg.rf.e4682497eda25d2d639e83d1d655d4fc.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/train/b7fcd547-60d072afa40cf3d4e89a928913630124_jpeg.rf.a49d2a003e30c8b32865f3d43f4933bb.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/train/846cf55e-42_jpg.rf.ad0ad373bdc619d1e1771b549c72f4cb.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/train/4c9f0301-monowheel_12_jpg.rf.dbdfdfa6a003e1aea4ab9e69aa208def.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/train/4e42ee0c-_-_-_10_jpg.rf.d8cb4a8f3ecd5146efd79270376b954d.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/train/ant_sales-1030_png_jpg.rf.e6122844adc05593c86d492d2b818543.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/valid/image25_png_jpg.rf.8ad9ec59347c435970f2142ff7074cbe.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/valid/screenshot_17440_jpg.rf.0fa15167b99fcc4380da5e0433a63ed9.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/valid/screenshot_13131_jpg.rf.603f483cb49a9f7e300979b26f65e6e3.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/valid/screenshot_1341_jpg.rf.0e6e1343b17c6f262ac96e211636c427.jpg\",\n",
    "    \"Vehilce detection.v3i.coco/valid/image07_png_jpg.rf.4f472e89f7ba03ac51d8ed81c8c9e897.jpg\",\n",
    "    ]\n",
    "\n",
    "output_dir = cfg.OUTPUT_DIR_TEST_SET_EVALUATION\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for input_image_path in input_images:\n",
    "    im = Image.open(input_image_path)\n",
    "    im = np.array(im)\n",
    "    outputs = predictor(im)\n",
    "\n",
    "    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    vis_output = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "\n",
    "    output_image = vis_output.get_image()[:, :, ::-1]\n",
    "\n",
    "\n",
    "    output_image_path = os.path.join(output_dir, os.path.basename(input_image_path))\n",
    "    \n",
    " \n",
    "    Image.fromarray(output_image).save(output_image_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
